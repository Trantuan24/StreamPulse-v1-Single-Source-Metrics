# StreamPulse v1 - Real-time Data Pipeline

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![Apache Flink](https://img.shields.io/badge/Apache%20Flink-1.17-orange.svg)](https://flink.apache.org/)
[![Apache Kafka](https://img.shields.io/badge/Apache%20Kafka-7.4.0-red.svg)](https://kafka.apache.org/)
[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/Trantuan24/StreamPulse-v1-Single-Source-Metrics)

> **Level 1 - StreamMetrics Starter Project**
> A comprehensive real-time data pipeline for learning stream processing fundamentals

## ğŸ“‹ Tá»•ng quan

StreamPulse v1 lÃ  má»™t pipeline xá»­ lÃ½ dá»¯ liá»‡u thá»i gian thá»±c end-to-end Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cháº¡y trÃªn má»™t mÃ¡y duy nháº¥t. Dá»± Ã¡n nÃ y hoÃ n háº£o cho viá»‡c há»c táº­p vÃ  thá»±c hÃ nh cÃ¡c cÃ´ng nghá»‡ stream processing hiá»‡n Ä‘áº¡i.

### ğŸ¯ Má»¥c tiÃªu dá»± Ã¡n

- **Ingest**: Äá»c dá»¯ liá»‡u trip tá»« CSV/JSON vÃ  publish lÃªn Kafka
- **Processing**: Sá»­ dá»¥ng Apache Flink Ä‘á»ƒ xá»­ lÃ½ event-time vá»›i windowing vÃ  watermarks
- **Serving**: LÆ°u káº¿t quáº£ vÃ o Redis vÃ  expose qua FastAPI
- **Monitoring**: Theo dÃµi pipeline vá»›i Prometheus + Grafana

### ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
CSV Data â†’ Kafka â†’ Flink (Windowing) â†’ Redis â†’ FastAPI â†’ Grafana Dashboard
```

### ğŸ› ï¸ Technology Stack

- **Message Bus**: Apache Kafka (1 broker)
- **Stream Processing**: Apache Flink (1 JobManager, 1-2 TaskManagers)
- **Serving Layer**: Redis (key-value store)
- **API Layer**: FastAPI (Python)
- **Monitoring**: Prometheus + Grafana
- **Deployment**: Docker Compose

### SLO (Service Level Objectives)

- **Latency**: p95 end-to-end latency â‰¤ 3 giÃ¢y
- **Throughput**: Xá»­ lÃ½ Ä‘Æ°á»£c 1-3k events/giÃ¢y trÃªn laptop
- **Reliability**: Consumer lag â‰ˆ 0 trong Ã­t nháº¥t 5 phÃºt liÃªn tá»¥c
- **Recovery**: KhÃ´i phá»¥c tá»« checkpoint khi TaskManager bá»‹ kill

### Cáº¥u TrÃºc Dá»± Ãn

```
tripstream-analytics/
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ docker-compose/          # Docker Compose configurations
â”œâ”€â”€ streaming/
â”‚   â”œâ”€â”€ jobs/                    # Flink applications
â”‚   â””â”€â”€ schemas/                 # Data schemas (JSON/Avro)
â”œâ”€â”€ producers/
â”‚   â”œâ”€â”€ replay/                  # CSV/JSON replay producer
â”‚   â””â”€â”€ synthetic/               # Synthetic data generator
â”œâ”€â”€ serving/
â”‚   â””â”€â”€ api/                     # FastAPI application
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml           # Prometheus configuration
â”‚   â”œâ”€â”€ grafana-dashboards/      # Grafana dashboard definitions
â”‚   â””â”€â”€ alerts/                  # Alert rules
â”œâ”€â”€ ops/
â”‚   â”œâ”€â”€ Makefile                 # Automation scripts
â”‚   â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â””â”€â”€ data/                    # Sample data files
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### Prerequisites

- Docker vÃ  Docker Compose
- `make` command (hoáº·c cháº¡y trá»±c tiáº¿p tá»« `ops/Makefile`)
- Ãt nháº¥t 4GB RAM available

### Khá»Ÿi cháº¡y pipeline

```bash
# Clone repository
git clone https://github.com/Trantuan24/StreamPulse-v1-Single-Source-Metrics
cd StreamPulse-v1

# Khá»Ÿi Ä‘á»™ng toÃ n bá»™ pipeline
cd ops
make setup

# Gá»­i dá»¯ liá»‡u test
make produce-test

# Kiá»ƒm tra API
make test-api
```

### ğŸ”— Truy cáº­p cÃ¡c dá»‹ch vá»¥

| Service               | URL                        | Credentials |
| --------------------- | -------------------------- | ----------- |
| **Grafana Dashboard** | http://localhost:3000      | admin/admin |
| **Flink Web UI**      | http://localhost:8081      | -           |
| **Prometheus**        | http://localhost:9090      | -           |
| **FastAPI Docs**      | http://localhost:8000/docs | -           |

### Dá»«ng vÃ  dá»n dáº¹p

```bash
# Dá»«ng services
make down

# Dá»n dáº¹p hoÃ n toÃ n
make clean
```

## ğŸ“Š Data Schema & API

### Sample Data Schema

```json
{
  "trip_id": "trip_001",
  "region_id": 1,
  "event_time": "2025-08-15T10:30:00Z",
  "fare": 25.5,
  "duration": 1800,
  "distance": 12.5
}
```

### ğŸ”Œ API Endpoints

| Method | Endpoint                                | Description                            |
| ------ | --------------------------------------- | -------------------------------------- |
| `GET`  | `/metrics/region/{region_id}?window=1m` | Láº¥y metrics theo region vÃ  time window |
| `GET`  | `/health`                               | Health check endpoint                  |
| `GET`  | `/metrics`                              | Prometheus metrics endpoint            |
| `GET`  | `/docs`                                 | Interactive API documentation          |

### ğŸ“ˆ Monitoring Dashboards

- **Pipeline Overview**: Throughput, latency, error rates
- **Kafka Metrics**: Consumer lag, partition distribution
- **Flink Metrics**: Checkpoint status, backpressure, task utilization
- **Business Metrics**: Trip counts, average fare by region

## ğŸ”§ Development & Troubleshooting

### Development Workflow

1. **Local Development**: Sá»­ dá»¥ng Docker Compose Ä‘á»ƒ cháº¡y táº¥t cáº£ services
2. **Testing**: Unit tests cho Flink jobs, integration tests cho API
3. **Debugging**: Logs aggregation, metrics monitoring
4. **Performance Testing**: Load testing vá»›i synthetic data

### Common Issues & Solutions

| Issue                       | Solution                                        |
| --------------------------- | ----------------------------------------------- |
| **Kafka Connection Issues** | Kiá»ƒm tra port 9092, container networking        |
| **Flink Job Failures**      | Xem logs trong Flink UI (http://localhost:8081) |
| **High Latency**            | Monitor backpressure, checkpoint duration       |
| **Data Loss**               | Verify checkpoint configuration vÃ  recovery     |
| **Out of Memory**           | TÄƒng memory limits trong docker-compose.yml     |

### Performance Tuning

- **Flink**: Adjust parallelism, checkpoint intervals
- **Kafka**: Tune batch size, compression
- **Redis**: Configure memory policies, persistence

## ğŸ¯ SLO (Service Level Objectives)

- **Latency**: p95 end-to-end latency â‰¤ 3 giÃ¢y
- **Throughput**: Xá»­ lÃ½ Ä‘Æ°á»£c 1-3k events/giÃ¢y trÃªn laptop
- **Reliability**: Consumer lag â‰ˆ 0 trong Ã­t nháº¥t 5 phÃºt liÃªn tá»¥c
- **Recovery**: KhÃ´i phá»¥c tá»« checkpoint khi TaskManager bá»‹ kill

## ğŸš€ Next Steps

Sau khi hoÃ n thÃ nh Level 1, cÃ³ thá»ƒ tiáº¿n lÃªn:

- **Level 2**: MultiStream Intelligence (Interval joins, OLAP, Schema Registry)
- **Level 3**: CloudStream Platform (Kubernetes, HA, Blue-Green deployment)

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Apache Flink Community
- Confluent Platform
- FastAPI Framework
- Grafana Labs

---
